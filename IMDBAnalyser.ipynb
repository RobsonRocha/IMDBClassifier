{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Sentimental analyser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first project from Machine Learning Engineer Nanodegree Program Udacity course\n",
    "(https://www.udacity.com/course/machine-learning-engineer-nanodegree--nd009t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first part of getting data and preparing it like the course's code in https://github.com/udacity/sagemaker-deployment/blob/master/Tutorials/IMDB%20Sentiment%20Analysis%20-%20XGBoost%20-%20Web%20App.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dowloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-14 20:07:49--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolvendo ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Conectando-se a ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... conectado.\n",
      "A requisição HTTP foi enviada, aguardando resposta... 200 OK\n",
      "Tamanho: 84125825 (80M) [application/x-gzip]\n",
      "Salvando em: “data/aclImdb_v1.tar.gz”\n",
      "\n",
      "data/aclImdb_v1.tar 100%[===================>]  80,23M  8,37MB/s    em 15s     \n",
      "\n",
      "2020-09-14 20:08:04 (5,43 MB/s) - “data/aclImdb_v1.tar.gz” salvo [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%mkdir -p data/all_data\n",
    "!wget -O data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -zxf data/aclImdb_v1.tar.gz -C data\n",
    "\n",
    "data_dir=\"data/all_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def read_imdb_data(data_dir='data/aclImdb'):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    \n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "        \n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            \n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            for f in files:\n",
    "                with open(f) as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    # Here we represent a positive review by '1' and a negative review by '0'\n",
    "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
    "                    \n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "                \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
     ]
    }
   ],
   "source": [
    "data, labels = read_imdb_data()\n",
    "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
    "            len(data['train']['pos']), len(data['train']['neg']),\n",
    "            len(data['test']['pos']), len(data['test']['neg'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_imdb_data(data, labels):\n",
    "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
    "    \n",
    "    #Combine positive and negative reviews and labels\n",
    "    data_train = data['train']['pos'] + data['train']['neg']\n",
    "    data_test = data['test']['pos'] + data['test']['neg']\n",
    "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
    "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
    "    \n",
    "    #Shuffle reviews and corresponding labels within training and test sets\n",
    "    data_train, labels_train = shuffle(data_train, labels_train)\n",
    "    data_test, labels_test = shuffle(data_test, labels_test)\n",
    "    \n",
    "    # Return a unified training data, test data, training labels, test labets\n",
    "    return data_train, data_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb reviews (combined): train = 25000 test = 25000\n",
      "When an attempt is made to assassinate the Emir of Ohtar, an Arab potentate visiting Washington, D.C., his life is saved by a cocktail waitress named Sunny Davis. Sunny becomes a national heroine and media celebrity and as a reward is offered a job working for the Protocol Section of the United States Department of State. Unknown to her however, the State Department officials who offer her the job have a hidden agenda.<br /><br />A map we see shows Ohtar lying on the borders of Saudi Arabia and South Yemen, in an area of barren desert known as the Rub al-Khali, or Empty Quarter. In real life a state in this location would have a population of virtually zero, and virtually zero strategic value, but for the purposes of the film we have to accept that Ohtar is of immense strategic importance in the Cold War and that the American government, who are keen to build a military base there, need to do all that they can in order to keep on the good side of its ruler. It transpires that the Emir has taken a fancy to the attractive young woman who saved him and he has reached a deal with the State Department; they can have their base provided that he can have Sunny as the latest addition to his harem. Sunny's new job is just a ruse to ensure that the Emir has further opportunities to meet her.<br /><br />A plot like this could have been the occasion for some hilarious satire, but in fact the film's satirical content is rather toned down. Possibly in 1984 the American public were not in the mood for trenchant satire on their country's foreign policy; this was, after all, the year in which Ronald Reagan carried forty-nine out of fifty states in the Presidential election and his hard line with the Soviet Union was clearly going down well with the voters. (If the film had been made a couple of years later, in the wake of the Iran/Contra affair, its tone might have been different).<br /><br />The film is not so much a satire as a vehicle for Goldie Hawn to show off her brand of cuteness and charm. Sunny is a typical Goldie character- pretty, sweet-natured, naive and not too bright. There is, however, a limit to how far you can go with cuteness and charm alone, and you cannot automatically make a bad film a good one just by making the leading character a dumb blonde. (Actually, that sounds more like a recipe for making a good film a bad one). Goldie tries her best to save this one, but never succeeds. Part of the reason is the inconsistent way in which her character is portrayed. On the one hand Sunny is a sweet, innocent country girl from Oregon. On the other hand she is a 35-year-old woman who works in a sleazy bar and wears a revealing costume. The effect is rather like imagining Rebecca of Sunnybrook Farm grown up and working as a Bunny Girl.<br /><br />The more important reason why Goldie is unable to rescue this film is even the best comedian or comedienne is no better than his/her material, and \"Protocol\" is simply unfunny. Whatever humour exists is tired and strained, relying on offensive stereotypes about Arab men who, apparently, all lust after Western women, particularly if they are blonde and blue-eyed. There was a lot of this sort of thing about in the mid-eighties, as this was the period which also saw the awful Ben Kingsley/ Nastassia Kinski film \"Harem\", about a lascivious Middle Eastern ruler who kidnaps a young American woman, and the mini-series of the same name which told a virtually identical story with a period setting. The film-makers seem to have realised that their film would not work as a pure comedy, because towards the end it turns into a sort of latter-day \"Mr Smith Goes to Washington\". Sunny turns from a blonde bimbo into a fount of political wisdom and starts uttering all sorts of platitudes about Democracy and the Constitution and the Citizen's Duty to Vote and We The People and how the Price of Liberty is Eternal Vigilance blah blah blah",
      "",
      ", but in truth the film is no more successful as a political parable than it is as a comedy.<br /><br />Goldie Hawn has made a number of good comedies, such as \"Cactus Flower\", \"Overboard\" and \"\"Housesitter\", but \"Protocol\" is not one of them. I have not seen all of her films, but of those I have seen this dire comedy is by far the worst. 3/10\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\n",
    "print(\"IMDb reviews (combined): train = {} test = {}\".format(len(train_X), len(test_X)))\n",
    "print(train_X[540])\n",
    "print(train_y[540])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering the data\n",
    "\n",
    "### Answering the first question\n",
    "\n",
    "#### What does review_to_words do?\n",
    "\n",
    "Besides tokenize each word, it clean and put them in lower case for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/robson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/robson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    import re\n",
    "    clean = re.compile(r'<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "# That's my review_to_words function\n",
    "def filter_text_list(text_list):\n",
    "    cleanr = re.compile(r\"[^a-zA-Z0-9]\")\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    \n",
    "    filtered_sentence = []\n",
    "    docs = []\n",
    "    for text in text_list:\n",
    "        word_tokens = word_tokenize(re.sub(cleanr, ' ', remove_html_tags(text.lower()))) # All words in lower case\n",
    "        filtered_sentence.append([w for w in word_tokens if not w in stop_words])   \n",
    "        \n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train_X length = 25000 - filtered_train_X length = 25000\n",
      "Original test_X length = 25000 - filtered_test_X length = 25000\n"
     ]
    }
   ],
   "source": [
    "train_filtered_X = filter_text_list(train_X)\n",
    "# Just checking\n",
    "print(('Original train_X length = {} - filtered_train_X length = {}').format(len(train_X),len(train_filtered_X)))\n",
    "\n",
    "test_filtered_X = filter_text_list(test_X)\n",
    "# Just checking\n",
    "print(('Original test_X length = {} - filtered_test_X length = {}').format(len(test_X),len(test_filtered_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['attempt', 'made', 'assassinate', 'emir', 'ohtar', 'arab', 'potentate', 'visiting', 'washington', 'c', 'life', 'saved', 'cocktail', 'waitress', 'named', 'sunny', 'davis', 'sunny', 'becomes', 'national', 'heroine', 'media', 'celebrity', 'reward', 'offered', 'job', 'working', 'protocol', 'section', 'united', 'states', 'department', 'state', 'unknown', 'however', 'state', 'department', 'officials', 'offer', 'job', 'hidden', 'agenda', 'map', 'see', 'shows', 'ohtar', 'lying', 'borders', 'saudi', 'arabia', 'south', 'yemen', 'area', 'barren', 'desert', 'known', 'rub', 'al', 'khali', 'empty', 'quarter', 'real', 'life', 'state', 'location', 'would', 'population', 'virtually', 'zero', 'virtually', 'zero', 'strategic', 'value', 'purposes', 'film', 'accept', 'ohtar', 'immense', 'strategic', 'importance', 'cold', 'war', 'american', 'government', 'keen', 'build', 'military', 'base', 'need', 'order', 'keep', 'good', 'side', 'ruler', 'transpires', 'emir', 'taken', 'fancy', 'attractive', 'young', 'woman', 'saved', 'reached', 'deal', 'state', 'department', 'base', 'provided', 'sunny', 'latest', 'addition', 'harem', 'sunny', 'new', 'job', 'ruse', 'ensure', 'emir', 'opportunities', 'meet', 'plot', 'like', 'could', 'occasion', 'hilarious', 'satire', 'fact', 'film', 'satirical', 'content', 'rather', 'toned', 'possibly', '1984', 'american', 'public', 'mood', 'trenchant', 'satire', 'country', 'foreign', 'policy', 'year', 'ronald', 'reagan', 'carried', 'forty', 'nine', 'fifty', 'states', 'presidential', 'election', 'hard', 'line', 'soviet', 'union', 'clearly', 'going', 'well', 'voters', 'film', 'made', 'couple', 'years', 'later', 'wake', 'iran', 'contra', 'affair', 'tone', 'might', 'different', 'film', 'much', 'satire', 'vehicle', 'goldie', 'hawn', 'show', 'brand', 'cuteness', 'charm', 'sunny', 'typical', 'goldie', 'character', 'pretty', 'sweet', 'natured', 'naive', 'bright', 'however', 'limit', 'far', 'go', 'cuteness', 'charm', 'alone', 'automatically', 'make', 'bad', 'film', 'good', 'one', 'making', 'leading', 'character', 'dumb', 'blonde', 'actually', 'sounds', 'like', 'recipe', 'making', 'good', 'film', 'bad', 'one', 'goldie', 'tries', 'best', 'save', 'one', 'never', 'succeeds', 'part', 'reason', 'inconsistent', 'way', 'character', 'portrayed', 'one', 'hand', 'sunny', 'sweet', 'innocent', 'country', 'girl', 'oregon', 'hand', '35', 'year', 'old', 'woman', 'works', 'sleazy', 'bar', 'wears', 'revealing', 'costume', 'effect', 'rather', 'like', 'imagining', 'rebecca', 'sunnybrook', 'farm', 'grown', 'working', 'bunny', 'girl', 'important', 'reason', 'goldie', 'unable', 'rescue', 'film', 'even', 'best', 'comedian', 'comedienne', 'better', 'material', 'protocol', 'simply', 'unfunny', 'whatever', 'humour', 'exists', 'tired', 'strained', 'relying', 'offensive', 'stereotypes', 'arab', 'men', 'apparently', 'lust', 'western', 'women', 'particularly', 'blonde', 'blue', 'eyed', 'lot', 'sort', 'thing', 'mid', 'eighties', 'period', 'also', 'saw', 'awful', 'ben', 'kingsley', 'nastassia', 'kinski', 'film', 'harem', 'lascivious', 'middle', 'eastern', 'ruler', 'kidnaps', 'young', 'american', 'woman', 'mini', 'series', 'name', 'told', 'virtually', 'identical', 'story', 'period', 'setting', 'film', 'makers', 'seem', 'realised', 'film', 'would', 'work', 'pure', 'comedy', 'towards', 'end', 'turns', 'sort', 'latter', 'day', 'mr', 'smith', 'goes', 'washington', 'sunny', 'turns', 'blonde', 'bimbo', 'fount', 'political', 'wisdom', 'starts', 'uttering', 'sorts', 'platitudes', 'democracy', 'constitution', 'citizen', 'duty', 'vote', 'people', 'price', 'liberty', 'eternal', 'vigilance', 'blah', 'blah', 'blah', 'truth', 'film', 'successful', 'political', 'parable', 'comedy', 'goldie', 'hawn', 'made', 'number', 'good', 'comedies', 'cactus', 'flower', 'overboard', 'housesitter', 'protocol', 'one', 'seen', 'films', 'seen', 'dire', 'comedy', 'far', 'worst', '3', '10']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train_filtered_X[540])\n",
    "print(train_y[540])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Before ###\n",
      "I remember hitch hiking to Spain at 25, getting a lift from, what turned out to be, two fleeing Italian small crooks. They were doing a lot outside the law, but from the other side carrying a little portrait of Jesus in the pocket for their protection...Just and unjust, good and bad, criminal and correct where here in a new combination, outside of the categories I used to know. 'Les Valseuses' gives me, although a film and not real life, a picture close to my own experiences: the intenseness of each moment as soon as you leave 'all behind' and go for the momentous, whatever comes your way, it's another state of mind and also 'dangerous' form of life, because, as we all know, there are people who are not ready for this and willing to persecute you for 'stealing' and so on...This film touches 'values', it's a story about 'what's right and wrong': morals. It's resurrection of the individual fighting him/ herself free against the 'false morals' and conformism...There's danger all the way, because, how far can you go with your own 'freedom' and crossing your own moral borders and that of other people? What to do with people who are willing to hurt you, put you in jail or even shoot at you for the things that you do, like \"stealing\" some petrol from a multinational oil company for you fifth hand car? Les Valseuses re-awakens these questions in me, because morality, in contradiction to the usual 'media message', is quite complex...\n",
      "### After ###\n",
      "['remember', 'hitch', 'hiking', 'spain', '25', 'getting', 'lift', 'turned', 'two', 'fleeing', 'italian', 'small', 'crooks', 'lot', 'outside', 'law', 'side', 'carrying', 'little', 'portrait', 'jesus', 'pocket', 'protection', 'unjust', 'good', 'bad', 'criminal', 'correct', 'new', 'combination', 'outside', 'categories', 'used', 'know', 'les', 'valseuses', 'gives', 'although', 'film', 'real', 'life', 'picture', 'close', 'experiences', 'intenseness', 'moment', 'soon', 'leave', 'behind', 'go', 'momentous', 'whatever', 'comes', 'way', 'another', 'state', 'mind', 'also', 'dangerous', 'form', 'life', 'know', 'people', 'ready', 'willing', 'persecute', 'stealing', 'film', 'touches', 'values', 'story', 'right', 'wrong', 'morals', 'resurrection', 'individual', 'fighting', 'free', 'false', 'morals', 'conformism', 'danger', 'way', 'far', 'go', 'freedom', 'crossing', 'moral', 'borders', 'people', 'people', 'willing', 'hurt', 'put', 'jail', 'even', 'shoot', 'things', 'like', 'stealing', 'petrol', 'multinational', 'oil', 'company', 'fifth', 'hand', 'car', 'les', 'valseuses', 'awakens', 'questions', 'morality', 'contradiction', 'usual', 'media', 'message', 'quite', 'complex']\n"
     ]
    }
   ],
   "source": [
    "# Just looking the diference\n",
    "before = train_X[37]\n",
    "after = train_filtered_X[37]\n",
    "print(\"### Before ###\")\n",
    "print(before)\n",
    "print(\"### After ###\")\n",
    "print(after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robson/.local/lib/python3.8/site-packages/numpy/lib/function_base.py:792: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, order=order, subok=subok, copy=True)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "train_sentences=np.copy(train_filtered_X)\n",
    "\n",
    "count = 0\n",
    "wordsDic = Counter() #Dictionary that will map a word to the number of times it appeared in all the training sentences\n",
    "for i, sentence in enumerate(train_sentences):\n",
    "    #The sentences will be stored as a list of words/tokens\n",
    "    train_sentences[i] = []\n",
    "    for word in nltk.word_tokenize(' '.join(sentence)): #Tokenizing the words\n",
    "        train_sentences[i].append(word)\n",
    "        wordsDic.update([word])\n",
    "        count=count+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 used words \n",
    "\n",
    "### Answering the second question\n",
    "\n",
    "#### What are the five most frequently appearing words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 of words : ['movie', 'film', 'one', 'like', 'good']\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "dicSorted = {}\n",
    "\n",
    "for k in sorted(wordsDic, key=wordsDic.get, reverse=True):\n",
    "    dicSorted[k]=wordsDic[k]\n",
    "    if i == 4:\n",
    "        break\n",
    "    i = i + 1\n",
    "    \n",
    "print(\"Top 5 of words : {}\".format(list(dicSorted)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The third question\n",
    "\n",
    "#### Create a word dictionary\n",
    "\n",
    "The variable words2index is the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsList = {k:v for k,v in wordsDic.items() if v>1}\n",
    "wordsList = sorted(wordsList, key=wordsList.get, reverse=True)\n",
    "# Creating unknown and padding index \n",
    "wordsList = ['FILL_','UNKN_'] + wordsList\n",
    "words2index = {o:i for i,o in enumerate(wordsList)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The fourth question\n",
    "\n",
    "Padding process and the function redefineSentences finish the process truncating to 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying unknown and padding index\n",
    "for i, sentence in enumerate(train_filtered_X):\n",
    "    train_filtered_X[i] = [words2index[word] if word in words2index else words2index['UNKN_'] for word in sentence]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(test_filtered_X):\n",
    "    sentence = re.sub(\"[^a-zA-Z]\",  \" \", str(sentence))\n",
    "    test_filtered_X[i] = [words2index[word] if word in words2index else words2index['UNKN_'] for word in nltk.word_tokenize(sentence)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 2, 141, 29, 178, 317, 2, 89, 156, 173, 1200, 10649, 32555, 5, 1948, 989, 12824, 56, 404, 23, 241, 22, 755, 69, 25, 49, 1200, 107, 6955, 3642, 21157, 4197, 11, 1421, 85, 5, 127, 50, 869, 15, 227, 1379, 767, 1, 11, 224, 35, 109, 26, 65, 11, 21, 171, 80, 623, 248, 181, 1300, 15, 249, 810, 109, 26, 416, 50, 37748, 977, 141, 29, 1218, 758, 5, 2, 42, 410, 37749, 878, 141, 29]\n",
      "0\n",
      "[1397, 208, 2, 4113, 3773, 1198, 878, 4695, 225, 6123, 77, 790, 1282, 5, 9447, 197, 269, 180, 36, 120, 339, 247, 1405, 827, 1103, 1759, 1509, 1397, 705, 17, 1841, 10714, 127, 819, 4, 58, 10283, 124, 213, 25, 4345, 350, 20, 17, 154, 68, 112, 661, 758, 33, 2726, 11615, 2897, 481, 33, 6, 11615, 1397, 31, 4]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Just checking\n",
    "print(train_filtered_X[45])\n",
    "print(train_y[45])\n",
    "print(test_filtered_X[45])\n",
    "print(test_y[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redefineSentences(sentences, seq_len):\n",
    "    redefinedSentences = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            redefinedSentences[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return redefinedSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 200 #The length that the sentences will be cut\n",
    "\n",
    "train_filtered_X = redefineSentences(train_filtered_X, seq_len)\n",
    "test_filtered_X = redefineSentences(test_filtered_X, seq_len)\n",
    "\n",
    "train_y = np.array(train_y)\n",
    "test_y = np.array(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0    13     2   141    29   178   317     2    89   156   173  1200\n",
      " 10649 32555     5  1948   989 12824    56   404    23   241    22   755\n",
      "    69    25    49  1200   107  6955  3642 21157  4197    11  1421    85\n",
      "     5   127    50   869    15   227  1379   767     1    11   224    35\n",
      "   109    26    65    11    21   171    80   623   248   181  1300    15\n",
      "   249   810   109    26   416    50 37748   977   141    29  1218   758\n",
      "     5     2    42   410 37749   878   141    29]\n",
      "0\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0  1397   208     2  4113\n",
      "  3773  1198   878  4695   225  6123    77   790  1282     5  9447   197\n",
      "   269   180    36   120   339   247  1405   827  1103  1759  1509  1397\n",
      "   705    17  1841 10714   127   819     4    58 10283   124   213    25\n",
      "  4345   350    20    17   154    68   112   661   758    33  2726 11615\n",
      "  2897   481    33     6 11615  1397    31     4]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Just checking\n",
    "print(train_filtered_X[45])\n",
    "print(train_y[45])\n",
    "print(test_filtered_X[45])\n",
    "print(test_y[45])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and upload the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-2\n",
      "sagemaker-us-east-2-214237513994\n",
      "CPU times: user 543 ms, sys: 115 ms, total: 658 ms\n",
      "Wall time: 2.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.Session(boto3.Session())\n",
    "print(region)\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "print(bucket)\n",
    "prefix = 'sagemaker/IMDB-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_s3(local_directory, work_directory):\n",
    "    return sagemaker_session.upload_data(local_directory, key_prefix=work_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test length = 21250 - y_test length = 21250\n",
      "X_valid length = 3750 - y_valid length = 3750\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividing test data in test and validation\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(test_filtered_X, test_y, test_size=0.85, random_state=42)\n",
    "\n",
    "# Just checking\n",
    "print(('X_test length = {} - y_test length = {}').format(len(X_test),len(y_test)))\n",
    "print(('X_valid length = {} - y_valid length = {}').format(len(X_valid),len(y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test length = 1250 - y_test length = 1250\n",
      "X_valid length = 23750 - y_valid length = 23750\n"
     ]
    }
   ],
   "source": [
    "X_train, X_train1, y_train, y_train1 = train_test_split(train_filtered_X, train_y, test_size=0.95, random_state=42)\n",
    "\n",
    "print(('X_test length = {} - y_test length = {}').format(len(X_train),len(y_train)))\n",
    "print(('X_valid length = {} - y_valid length = {}').format(len(X_train1),len(y_train1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "    \n",
    "pd.concat([pd.DataFrame(train_y), pd.DataFrame(train_filtered_X)], axis=1) \\\n",
    "        .to_csv(os.path.join(data_dir, 'train'), header=False, index=False)\n",
    "\n",
    "pd.concat([pd.DataFrame(y_test), pd.DataFrame(X_test)], axis=1) \\\n",
    "        .to_csv(os.path.join(data_dir, 'test'), header=False, index=False)\n",
    "\n",
    "\n",
    "pd.concat([pd.DataFrame(y_valid), pd.DataFrame(X_valid)], axis=1) \\\n",
    "        .to_csv(os.path.join(data_dir, 'valid'), header=False, index=False)\n",
    "\n",
    "with open(os.path.join(data_dir, 'dictionary.dic'), 'wb') as f:\n",
    "    torch.save(words2index, f)              \n",
    "\n",
    "#Saving all data.\n",
    "s3_input_train = write_to_s3(data_dir, prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(source_dir ='./RNN',\n",
    "                    entry_point='train.py',\n",
    "                    role=role,\n",
    "                    sagemaker_session = sagemaker_session,\n",
    "                    framework_version='1.5.0',\n",
    "                    train_instance_count=1,\n",
    "                    py_version='py3',\n",
    "                    train_instance_type='ml.m5.xlarge',\n",
    "                    hyperparameters={\n",
    "                        'epochs'               : 10,\n",
    "                        'n_layers'             : 2,\n",
    "                        'embedding_dim'        : 400,\n",
    "                        'hidden_dim'           : 512,\n",
    "                        'vocab_size'           : len(words2index)+1,\n",
    "                        'dictionary_file_name' : 'dictionary.dic',\n",
    "                    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-14 12:26:41 Starting - Starting the training job...\n",
      "2020-09-14 12:26:42 Starting - Launching requested ML instances......\n",
      "2020-09-14 12:27:47 Starting - Preparing the instances for training...\n",
      "2020-09-14 12:28:37 Downloading - Downloading input data\n",
      "2020-09-14 12:28:37 Training - Downloading the training image..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-09-14 12:29:03,626 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-09-14 12:29:03,629 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-09-14 12:29:03,639 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-09-14 12:29:04,294 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-09-14 12:29:04,728 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-09-14 12:29:04,728 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-09-14 12:29:04,728 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-09-14 12:29:04,728 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmppc2wvakk/module_dir\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name\n",
      "  Building wheel for default-user-module-name (setup.py): started\n",
      "  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=13095 sha256=1e781ce09a0a36a557513a54683bc649b1acb305c4202b4076b5ec662f589815\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2hy9z7dv/wheels/6c/85/1c/29959a52daf00efee4cd698186e577b30ea62362ec18c8e626\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name\u001b[0m\n",
      "\u001b[34mInstalling collected packages: default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed default-user-module-name-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.1; however, version 20.2.3 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-09-14 12:29:06,687 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-09-14 12:29:06,701 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-09-14 12:29:06,714 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-09-14 12:29:06,725 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"hidden_dim\": 512,\n",
      "        \"dictionary_file_name\": \"dictionary.dic\",\n",
      "        \"embedding_dim\": 400,\n",
      "        \"vocab_size\": 46816,\n",
      "        \"epochs\": 10,\n",
      "        \"n_layers\": 2\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2020-09-14-12-26-38-178\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-214237513994/pytorch-training-2020-09-14-12-26-38-178/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"dictionary_file_name\":\"dictionary.dic\",\"embedding_dim\":400,\"epochs\":10,\"hidden_dim\":512,\"n_layers\":2,\"vocab_size\":46816}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-214237513994/pytorch-training-2020-09-14-12-26-38-178/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"dictionary_file_name\":\"dictionary.dic\",\"embedding_dim\":400,\"epochs\":10,\"hidden_dim\":512,\"n_layers\":2,\"vocab_size\":46816},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-09-14-12-26-38-178\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-214237513994/pytorch-training-2020-09-14-12-26-38-178/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--dictionary_file_name\",\"dictionary.dic\",\"--embedding_dim\",\"400\",\"--epochs\",\"10\",\"--hidden_dim\",\"512\",\"--n_layers\",\"2\",\"--vocab_size\",\"46816\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_DIM=512\u001b[0m\n",
      "\u001b[34mSM_HP_DICTIONARY_FILE_NAME=dictionary.dic\u001b[0m\n",
      "\u001b[34mSM_HP_EMBEDDING_DIM=400\u001b[0m\n",
      "\u001b[34mSM_HP_VOCAB_SIZE=46816\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_HP_N_LAYERS=2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train.py --dictionary_file_name dictionary.dic --embedding_dim 400 --epochs 10 --hidden_dim 512 --n_layers 2 --vocab_size 46816\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mCollecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from nltk) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from nltk) (0.14.1)\u001b[0m\n",
      "\u001b[34mCollecting regex\n",
      "  Downloading regex-2020.7.14-cp36-cp36m-manylinux2010_x86_64.whl (660 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from nltk) (4.42.1)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py): started\u001b[0m\n",
      "\n",
      "2020-09-14 12:29:02 Training - Training image download completed. Training in progress.\u001b[34m  Building wheel for nltk (setup.py): finished with status 'done'\n",
      "  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434677 sha256=d1e2a683268081ff6a5235eaba3b6db3f670d21c262430e3dadd9bb171876b82\n",
      "  Stored in directory: /root/.cache/pip/wheels/de/5e/42/64abaeca668161c3e2cecc24f864a8fc421e3d07a104fc8a51\u001b[0m\n",
      "\u001b[34mSuccessfully built nltk\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, nltk\u001b[0m\n",
      "\u001b[34mSuccessfully installed nltk-3.5 regex-2020.7.14\u001b[0m\n",
      "\u001b[34mGPU not available, CPU used\u001b[0m\n",
      "\u001b[34mUsing device cpu.\u001b[0m\n",
      "\u001b[34mTrain dataset len = 25000\u001b[0m\n",
      "\u001b[34mTensor train dataset <torch.utils.data.dataset.TensorDataset object at 0x7fdeb3b95198>\u001b[0m\n",
      "\u001b[34mInitializing training.\u001b[0m\n",
      "\u001b[34m[2020-09-14 12:29:15.368 algo-1:43 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-09-14 12:29:15.368 algo-1:43 INFO hook.py:183] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-09-14 12:29:15.368 algo-1:43 INFO hook.py:228] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-09-14 12:29:15.369 algo-1:43 INFO hook.py:364] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2020-09-14 12:29:15.369 algo-1:43 INFO hook.py:422] Hook is writing from the hook with pid: 43\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mEpoch: 1/10... Step: 61... Loss: 0.484309... Val Loss: 0.550715\u001b[0m\n",
      "\u001b[34mValidation loss decreased (inf --> 0.550715).Saving model...\u001b[0m\n",
      "\u001b[34mEpoch: 2/10... Step: 100... Loss: 0.368256... Val Loss: 0.488107\u001b[0m\n",
      "\u001b[34mValidation loss decreased (0.550715 --> 0.488107).Saving model...\u001b[0m\n",
      "\u001b[34mEpoch: 4/10... Step: 200... Loss: 0.125930... Val Loss: 0.540894\u001b[0m\n",
      "\u001b[34mEpoch: 5/10... Step: 300... Loss: 0.097641... Val Loss: 0.699354\u001b[0m\n",
      "\u001b[34mEpoch: 7/10... Step: 400... Loss: 0.011532... Val Loss: 0.764596\u001b[0m\n",
      "\u001b[34mEpoch: 9/10... Step: 500... Loss: 0.004461... Val Loss: 0.812873\u001b[0m\n",
      "\u001b[34mEpoch: 10/10... Step: 600... Loss: 0.015451... Val Loss: 0.790113\u001b[0m\n",
      "\n",
      "2020-09-14 15:09:04 Uploading - Uploading generated training model\u001b[34m[2020-09-14 15:09:01.513 algo-1:43 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m2020-09-14 15:09:02,008 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-09-14 15:09:21 Completed - Training job completed\n",
      "Training seconds: 9660\n",
      "Billable seconds: 9660\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': s3_input_train})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After 10 epochs...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3750 texts from validation dataset\n",
    "\n",
    "Epoch: 1/10... Step: 61... Loss: 0.484309... Val Loss: 0.550715\n",
    "\n",
    "Validation loss decreased (inf --> 0.550715).Saving model...\n",
    "\n",
    "Epoch: 2/10... Step: 100... Loss: 0.368256... Val Loss: 0.488107\n",
    "\n",
    "Validation loss decreased (0.550715 --> 0.488107).Saving model...\n",
    "\n",
    "Epoch: 4/10... Step: 200... Loss: 0.125930... Val Loss: 0.540894\n",
    "\n",
    "Epoch: 5/10... Step: 300... Loss: 0.097641... Val Loss: 0.699354\n",
    "\n",
    "Epoch: 7/10... Step: 400... Loss: 0.011532... Val Loss: 0.764596\n",
    "\n",
    "Epoch: 9/10... Step: 500... Loss: 0.004461... Val Loss: 0.812873\n",
    "\n",
    "Epoch: 10/10... Step: 600... Loss: 0.015451... Val Loss: 0.790113"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first way that I'd like to use is download the trained model file and see the its behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the trained model file\n",
    "from RNN.RNN import IMDBClassifier\n",
    "import boto3\n",
    "import botocore\n",
    "import tarfile\n",
    "\n",
    "training_job_name = estimator.latest_training_job.name\n",
    "desc = sagemaker_session.sagemaker_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "trained_model_location = desc['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "modelFile=trained_model_location.replace(\"s3://{}\".format(bucket),\"\",1)\n",
    "\n",
    "modelFile=modelFile.replace(\"/\",\"\",1)\n",
    "\n",
    "fileName = modelFile.split(\"/\")[-1]\n",
    "\n",
    "s3 = boto3.Session(profile_name='robsonrocha').resource('s3')\n",
    "\n",
    "s3.Bucket(bucket).download_file(modelFile, fileName)\n",
    "\n",
    "tar = tarfile.open(fileName, \"r:gz\")\n",
    "tar.extractall()\n",
    "tar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### MODEL CONFIG {'embedding_dim': 400, 'hidden_dim': 512, 'vocab_size': 46816, 'output_size': 1, 'n_layers': 2, 'device': 'cpu', 'batch_size': 400}\n",
      "<class 'RNN.RNN.IMDBClassifier'>\n",
      "____\n",
      "IMDBClassifier(\n",
      "  (embedding): Embedding(46816, 400, padding_idx=0)\n",
      "  (lstm): LSTM(400, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "____\n"
     ]
    }
   ],
   "source": [
    "# Loading downloaded model\n",
    "\n",
    "modelCfg = {}    \n",
    "model_cfg = './model.cfg'\n",
    "with open(model_cfg, 'rb') as f:                                \n",
    "    modelCfg = torch.load(f)  \n",
    "\n",
    "print(\"### MODEL CONFIG {}\".format(modelCfg))\n",
    "\n",
    "completeModel = IMDBClassifier(modelCfg['vocab_size'], modelCfg['output_size'], modelCfg['embedding_dim'], modelCfg['hidden_dim'], modelCfg['n_layers'])    \n",
    "\n",
    "complete_model = './model.pth'\n",
    "with open(complete_model, 'rb') as f:\n",
    "    completeModel.load_state_dict(torch.load(f))\n",
    "\n",
    "complete_dict = './model.dic'\n",
    "with open(complete_dict, 'rb') as f:\n",
    "    completeModelDict = torch.load(f)\n",
    "\n",
    "completeModel.to(torch.device(modelCfg['device']))\n",
    "completeModel.dictionary = completeModelDict\n",
    "\n",
    "print(type(completeModel))\n",
    "print('____')\n",
    "print(completeModel)\n",
    "print('____')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.608\n",
      "Test accuracy: 71.073%\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "from RNN.data import Dataset\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")            \n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "dataset = Dataset(data_dir)\n",
    "\n",
    "test_loader = dataset.getDatasetTest(completeModel.batch_size)\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(completeModel.parameters(), lr=0.005)     \n",
    "\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "h = completeModel.init_hidden(completeModel.batch_size)\n",
    "completeModel.eval()\n",
    "for inputs, labels in test_loader:\n",
    "    h = tuple([each.data for each in h])\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    output, h = completeModel(inputs, h)\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    pred = torch.round(output.squeeze()) \n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "    \n",
    "\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second way is creating the endpoint and call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "training_job_name = estimator.latest_training_job.name\n",
    "desc = sagemaker_session.sagemaker_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "trained_model_location = desc['ModelArtifacts']['S3ModelArtifacts']\n",
    "model = PyTorchModel(model_data=trained_model_location,\n",
    "                     role=role,\n",
    "                     framework_version='1.5.0',\n",
    "                     entry_point='api.py',\n",
    "                     sagemaker_session=sagemaker_session,\n",
    "                     source_dir='./RNN') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "source": [
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_review = 'This movie was made to be watch with family in your weekend. I think it worth the time.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RNN.data import Dataset\n",
    "\n",
    "test = Dataset()\n",
    "\n",
    "text = test.transformRawData(words2index,[test_review], seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"algorithm\": \"RNN\", \"answer\": \"1\"}'\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "from sagemaker.session import Session\n",
    "\n",
    "\n",
    "runtime_client = boto3.Session().client('sagemaker-runtime')\n",
    "\n",
    "payload = '{\"text\":\"'+test_review+'\"}'\n",
    "\n",
    "response = runtime_client.invoke_endpoint(EndpointName=predictor.endpoint, \n",
    "                                   ContentType='application/json', \n",
    "                                   Body=payload)\n",
    "result = response['Body'].read()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
